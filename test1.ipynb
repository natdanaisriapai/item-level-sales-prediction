{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130add0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install pandas numpy scikit-learn plotly lightgbm optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "print(df_train.head())\n",
    "print(\"----------------------------------------\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74550c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53538c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initial Data Exploration\n",
    "print(\"\\nTraining Data Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Data Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(df_test.info())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing Values in Training Data:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nMissing Values in Test Data:\")\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nTraining Data Statistics:\")\n",
    "print(df_train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Engineering\n",
    "def create_features(df):\n",
    "    \"\"\"Create time-based features from date column\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # Lag features (for training data only)\n",
    "    if 'sales' in df.columns:\n",
    "        # Group by store and item\n",
    "        grouped = df.groupby(['store', 'item'])\n",
    "        \n",
    "        # Create lag features\n",
    "        df['sales_lag_7'] = grouped['sales'].transform(lambda x: x.shift(7))\n",
    "        df['sales_lag_14'] = grouped['sales'].transform(lambda x: x.shift(14))\n",
    "        df['sales_lag_30'] = grouped['sales'].transform(lambda x: x.shift(30))\n",
    "        \n",
    "        # Create rolling mean features\n",
    "        df['sales_rolling_mean_7'] = grouped['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "        df['sales_rolling_mean_14'] = grouped['sales'].transform(lambda x: x.rolling(window=14, min_periods=1).mean())\n",
    "        df['sales_rolling_mean_30'] = grouped['sales'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "df_train = create_features(df_train)\n",
    "df_test = create_features(df_test)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_store = LabelEncoder()\n",
    "le_item = LabelEncoder()\n",
    "\n",
    "df_train['store_encoded'] = le_store.fit_transform(df_train['store'])\n",
    "df_train['item_encoded'] = le_item.fit_transform(df_train['item'])\n",
    "\n",
    "df_test['store_encoded'] = le_store.transform(df_test['store'])\n",
    "df_test['item_encoded'] = le_item.transform(df_test['item'])\n",
    "\n",
    "# Define features for modeling\n",
    "base_features = ['year', 'month', 'day', 'day_of_week', 'quarter', 'store_encoded', 'item_encoded']\n",
    "lag_features = ['sales_lag_7', 'sales_lag_14', 'sales_lag_30',\n",
    "                'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_mean_30']\n",
    "\n",
    "# Final feature list for training (excluding early dates due to lag features)\n",
    "features = base_features + lag_features\n",
    "df_train = df_train.dropna()  # Remove rows with NaN from lag features\n",
    "\n",
    "print(\"\\nFeatures created:\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train-Validation-Test Split\n",
    "print(\"Preparing train-validation-test split...\")\n",
    "\n",
    "# Sort by date to prevent data leakage\n",
    "df_train = df_train.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Split into train, validation, test\n",
    "n_val = 45000\n",
    "n_test = 45000\n",
    "n_train = len(df_train) - n_val - n_test\n",
    "\n",
    "if len(df_train) < n_train + n_val + n_test:\n",
    "    raise ValueError(\"Not enough data in df_train to split into 3 sets of 45000 each.\")\n",
    "\n",
    "train_data = df_train.iloc[:n_train]\n",
    "val_data = df_train.iloc[n_train:n_train + n_val]\n",
    "test_data = df_train.iloc[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['sales']\n",
    "X_val = val_data[features]\n",
    "y_val = val_data['sales']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['sales']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Prepare test features for competition/test set\n",
    "X_test_competition = df_test[base_features]  # Note: lag features will be calculated during prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1af56a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 5. Model Training and Evaluation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"\"):\n",
    "    \"\"\"Calculate and print model evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    return rmse, mae, r2\n",
    "\n",
    "# สร้าง dictionary สำหรับเก็บโมเดลและผลลัพธ์\n",
    "models = {}\n",
    "\n",
    "# 1. LightGBM\n",
    "print(\"Training LightGBM model...\")\n",
    "lgbm_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "val_pred_lgbm = lgbm_model.predict(X_val)\n",
    "print(\"LightGBM Results:\")\n",
    "rmse_lgbm, mae_lgbm, r2_lgbm = evaluate_model(y_val, val_pred_lgbm, \"LightGBM Validation\")\n",
    "models['LightGBM'] = lgbm_model\n",
    "performance_results.append({\n",
    "    'Model': 'LightGBM',\n",
    "    'RMSE': rmse_lgbm,\n",
    "    'MAE': mae_lgbm,\n",
    "    'R2': r2_lgbm\n",
    "})\n",
    "\n",
    "# 2. RandomForestRegressor\n",
    "print(\"\\nTraining RandomForestRegressor model...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "val_pred_rf = rf_model.predict(X_val)\n",
    "print(\"RandomForestRegressor Results:\")\n",
    "rmse_rf, mae_rf, r2_rf = evaluate_model(y_val, val_pred_rf, \"RandomForest Validation\")\n",
    "models['RandomForest'] = rf_model\n",
    "performance_results.append({\n",
    "    'Model': 'RandomForest',\n",
    "    'RMSE': rmse_rf,\n",
    "    'MAE': mae_rf,\n",
    "    'R2': r2_rf\n",
    "})\n",
    "\n",
    "# 3. GradientBoostingRegressor\n",
    "print(\"\\nTraining GradientBoostingRegressor model...\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "val_pred_gb = gb_model.predict(X_val)\n",
    "print(\"GradientBoostingRegressor Results:\")\n",
    "rmse_gb, mae_gb, r2_gb = evaluate_model(y_val, val_pred_gb, \"GradientBoosting Validation\")\n",
    "models['GradientBoosting'] = gb_model\n",
    "performance_results.append({\n",
    "    'Model': 'GradientBoosting',\n",
    "    'RMSE': rmse_gb,\n",
    "    'MAE': mae_gb,\n",
    "    'R2': r2_gb\n",
    "})\n",
    "\n",
    "# แสดงตารางสรุป performance ของแต่ละโมเดล\n",
    "import pandas as pd\n",
    "performance_df = pd.DataFrame(performance_results)\n",
    "print(\"\\nSummary of performance of each model (Validation Set):\")\n",
    "display(performance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22919885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generate Predictions for Next 3 Months\n",
    "print(\"Generating predictions for next 3 months...\")\n",
    "\n",
    "# Create future dates for prediction\n",
    "last_date = df_train['date'].max()\n",
    "future_dates = pd.date_range(\n",
    "    start=last_date + timedelta(days=1),\n",
    "    end=last_date + timedelta(days=90),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Create prediction dataframe\n",
    "future_df = pd.DataFrame()\n",
    "future_df['date'] = future_dates\n",
    "\n",
    "# Create combinations of store and item\n",
    "store_items = df_train[['store', 'item']].drop_duplicates()\n",
    "future_predictions = []\n",
    "\n",
    "# Get the last 30 days of actual data for calculating lag features\n",
    "last_30_days = df_train[df_train['date'] > last_date - timedelta(days=30)].copy()\n",
    "\n",
    "# Generate predictions for each store-item combination\n",
    "for _, row in store_items.iterrows():\n",
    "    store, item = row['store'], row['item']\n",
    "    \n",
    "    # Create temporary dataframe for this store-item combination\n",
    "    temp_df = future_df.copy()\n",
    "    temp_df['store'] = store\n",
    "    temp_df['item'] = item\n",
    "    \n",
    "    # Get historical data for this store-item combination\n",
    "    hist_data = last_30_days[\n",
    "        (last_30_days['store'] == store) & \n",
    "        (last_30_days['item'] == item)\n",
    "    ].copy()\n",
    "    \n",
    "    # Initialize predictions list\n",
    "    all_predictions = []\n",
    "    current_data = hist_data.copy()\n",
    "    \n",
    "    # Predict day by day to properly calculate lag features\n",
    "    for date in future_dates:\n",
    "        # Create a single row for prediction\n",
    "        pred_row = pd.DataFrame({'date': [date], 'store': [store], 'item': [item]})\n",
    "        \n",
    "        # Apply basic feature engineering\n",
    "        pred_row = create_features(pred_row)\n",
    "        pred_row['store_encoded'] = le_store.transform([store])[0]\n",
    "        pred_row['item_encoded'] = le_item.transform([item])[0]\n",
    "        \n",
    "        # Calculate lag features using current_data\n",
    "        if len(current_data) >= 30:  # Ensure we have enough historical data\n",
    "            pred_row['sales_lag_7'] = current_data['sales'].iloc[-7]\n",
    "            pred_row['sales_lag_14'] = current_data['sales'].iloc[-14]\n",
    "            pred_row['sales_lag_30'] = current_data['sales'].iloc[-30]\n",
    "            \n",
    "            # Calculate rolling means\n",
    "            pred_row['sales_rolling_mean_7'] = current_data['sales'].iloc[-7:].mean()\n",
    "            pred_row['sales_rolling_mean_14'] = current_data['sales'].iloc[-14:].mean()\n",
    "            pred_row['sales_rolling_mean_30'] = current_data['sales'].iloc[-30:].mean()\n",
    "        else:\n",
    "            # If not enough history, use means from training data\n",
    "            store_item_means = df_train[\n",
    "                (df_train['store'] == store) & \n",
    "                (df_train['item'] == item)\n",
    "            ]['sales'].mean()\n",
    "            \n",
    "            pred_row['sales_lag_7'] = store_item_means\n",
    "            pred_row['sales_lag_14'] = store_item_means\n",
    "            pred_row['sales_lag_30'] = store_item_means\n",
    "            pred_row['sales_rolling_mean_7'] = store_item_means\n",
    "            pred_row['sales_rolling_mean_14'] = store_item_means\n",
    "            pred_row['sales_rolling_mean_30'] = store_item_means\n",
    "        \n",
    "        # Make predictions using all models\n",
    "        X_pred = pred_row[features]\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        pred_lgbm = models['LightGBM'].predict(X_pred)[0]\n",
    "        pred_rf = models['RandomForest'].predict(X_pred)[0]\n",
    "        pred_gb = models['GradientBoosting'].predict(X_pred)[0]\n",
    "        \n",
    "        # Calculate ensemble prediction (weighted average based on R2 scores)\n",
    "        weights = {\n",
    "            'LightGBM': r2_lgbm,\n",
    "            'RandomForest': r2_rf,\n",
    "            'GradientBoosting': r2_gb\n",
    "        }\n",
    "        total_weight = sum(weights.values())\n",
    "        weights = {k: v/total_weight for k, v in weights.items()}\n",
    "        \n",
    "        prediction = (\n",
    "            pred_lgbm * weights['LightGBM'] +\n",
    "            pred_rf * weights['RandomForest'] +\n",
    "            pred_gb * weights['GradientBoosting']\n",
    "        )\n",
    "        \n",
    "        # Store predictions\n",
    "        pred_row['predicted_sales'] = prediction\n",
    "        pred_row['predicted_sales_lgbm'] = pred_lgbm\n",
    "        pred_row['predicted_sales_rf'] = pred_rf\n",
    "        pred_row['predicted_sales_gb'] = pred_gb\n",
    "        \n",
    "        all_predictions.append(pred_row)\n",
    "        \n",
    "        # Update current_data with the new prediction for next iteration\n",
    "        pred_row['sales'] = prediction\n",
    "        current_data = pd.concat([current_data, pred_row])\n",
    "    \n",
    "    # Combine all predictions for this store-item\n",
    "    store_item_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    future_predictions.append(store_item_predictions)\n",
    "\n",
    "# Combine all predictions\n",
    "future_predictions_df = pd.concat(future_predictions, ignore_index=True)\n",
    "\n",
    "print(\"Predictions generated successfully!\")\n",
    "\n",
    "# Display sample of predictions with all model results\n",
    "print(\"\\nSample of predictions:\")\n",
    "cols_to_show = ['date', 'store', 'item', 'predicted_sales', \n",
    "                'predicted_sales_lgbm', 'predicted_sales_rf', 'predicted_sales_gb']\n",
    "print(future_predictions_df[cols_to_show].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Visualize Results\n",
    "def plot_predictions(actual_df, pred_df, store=None, item=None):\n",
    "    \"\"\"Plot actual vs predicted sales for specific store and item\"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Filter data if store and item are specified\n",
    "    if store is not None and item is not None:\n",
    "        actual_df = actual_df[\n",
    "            (actual_df['store'] == store) & \n",
    "            (actual_df['item'] == item)\n",
    "        ]\n",
    "        pred_df = pred_df[\n",
    "            (pred_df['store'] == store) & \n",
    "            (pred_df['item'] == item)\n",
    "        ]\n",
    "    \n",
    "    # Aggregate daily sales if showing all stores/items\n",
    "    actual_daily = actual_df.groupby('date')['sales'].sum().reset_index()\n",
    "    \n",
    "    # Plot actual sales\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=actual_daily['date'],\n",
    "        y=actual_daily['sales'],\n",
    "        name='Actual Sales',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    # Plot predictions from each model\n",
    "    colors = {\n",
    "        'predicted_sales': 'red',      # Ensemble\n",
    "        'predicted_sales_lgbm': 'green',  # LightGBM\n",
    "        'predicted_sales_rf': 'orange',   # Random Forest\n",
    "        'predicted_sales_gb': 'purple'    # Gradient Boosting\n",
    "    }\n",
    "    \n",
    "    names = {\n",
    "        'predicted_sales': 'Ensemble Prediction',\n",
    "        'predicted_sales_lgbm': 'LightGBM Prediction',\n",
    "        'predicted_sales_rf': 'Random Forest Prediction',\n",
    "        'predicted_sales_gb': 'Gradient Boosting Prediction'\n",
    "    }\n",
    "    \n",
    "    for pred_col, color in colors.items():\n",
    "        pred_daily = pred_df.groupby('date')[pred_col].sum().reset_index()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pred_daily['date'],\n",
    "            y=pred_daily[pred_col],\n",
    "            name=names[pred_col],\n",
    "            line=dict(\n",
    "                color=color,\n",
    "                dash='dash' if pred_col == 'predicted_sales' else 'dot'\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    title = 'Sales Prediction: '\n",
    "    title += f'Store {store}, Item {item}' if store and item else 'All Stores and Items'\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sales',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        width=1200,\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot overall predictions\n",
    "print(\"Creating visualizations...\")\n",
    "overall_plot = plot_predictions(val_data, future_predictions_df)\n",
    "overall_plot.show()\n",
    "\n",
    "# Example: Plot predictions for top 3 selling items\n",
    "print(\"\\nAnalyzing top selling items...\")\n",
    "top_items = df_train.groupby(['store', 'item'])['sales'].sum().sort_values(ascending=False).head(3)\n",
    "print(\"\\nTop 3 store-item combinations by total sales:\")\n",
    "for (store, item), sales in top_items.items():\n",
    "    print(f\"Store: {store}, Item: {item}, Total Sales: {sales:,.0f}\")\n",
    "    specific_plot = plot_predictions(val_data, future_predictions_df, store, item)\n",
    "    specific_plot.show()\n",
    "\n",
    "# Calculate and display model contribution to final predictions\n",
    "print(\"\\nAnalyzing model contributions to ensemble predictions...\")\n",
    "weights = {\n",
    "    'LightGBM': r2_lgbm,\n",
    "    'RandomForest': r2_rf,\n",
    "    'GradientBoosting': r2_gb\n",
    "}\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v/total_weight * 100 for k, v in weights.items()}\n",
    "\n",
    "print(\"\\nModel weights in ensemble prediction:\")\n",
    "for model, weight in weights.items():\n",
    "    print(f\"{model}: {weight:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
